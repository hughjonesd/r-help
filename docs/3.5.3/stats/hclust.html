<!DOCTYPE html><html><head><title>R: Hierarchical Clustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body> <div style='padding: 20pt; border: 2px solid black; text-align:center;'><b>This help topic is for R version 3.5.3. For the corresponding topic in the current version of R, see <a href='https://stat.ethz.ch/R-manual/R-patched/library/stats/html/hclust.html'>https://stat.ethz.ch/R-manual/R-patched/library/stats/html/hclust.html</a></b></div><div class="container">

<table style="width: 100%;"><tr><td>hclust {stats}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2 id='hclust'>Hierarchical Clustering</h2>

<h3>Description</h3>

<p>Hierarchical cluster analysis on a set of dissimilarities and
methods for analyzing it.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclust(d, method = "complete", members = NULL)

## S3 method for class 'hclust'
plot(x, labels = NULL, hang = 0.1, check = TRUE,
     axes = TRUE, frame.plot = FALSE, ann = TRUE,
     main = "Cluster Dendrogram",
     sub = NULL, xlab = NULL, ylab = "Height", ...)
</code></pre>


<h3>Arguments</h3>

<table>
<tr><td><code id="hclust_:_d">d</code></td>
<td>
<p>a dissimilarity structure as produced by <code>dist</code>.</p>
</td></tr>
<tr><td><code id="hclust_:_method">method</code></td>
<td>
<p>the agglomeration method to be used.  This should
be (an unambiguous abbreviation of) one of
<code>"ward.D"</code>, <code>"ward.D2"</code>, <code>"single"</code>, <code>"complete"</code>,
<code>"average"</code> (= UPGMA), <code>"mcquitty"</code> (= WPGMA),
<code>"median"</code> (= WPGMC) or <code>"centroid"</code> (= UPGMC).</p>
</td></tr>
<tr><td><code id="hclust_:_members">members</code></td>
<td>
<p><code>NULL</code> or a vector with length size of
<code>d</code>. See the &lsquo;Details&rsquo; section.</p>
</td></tr>
<tr><td><code id="hclust_:_x">x</code></td>
<td>
<p>an object of the type produced by <code>hclust</code>.</p>
</td></tr>
<tr><td><code id="hclust_:_hang">hang</code></td>
<td>
<p>The fraction of the plot height by which labels should hang
below the rest of the plot.
A negative value will cause the labels to hang down from 0.</p>
</td></tr>
<tr><td><code id="hclust_:_check">check</code></td>
<td>
<p>logical indicating if the <code>x</code> object should be
checked for validity.  This check is not necessary when <code>x</code>
is known to be valid such as when it is the direct result of
<code>hclust()</code>.  The default is <code>check=TRUE</code>, as invalid
inputs may crash <span class="rlang"><b>R</b></span> due to memory violation in the internal C
plotting code.</p>
</td></tr>
<tr><td><code id="hclust_:_labels">labels</code></td>
<td>
<p>A character vector of labels for the leaves of the
tree.  By default the row names or row numbers of the original data are
used.  If <code>labels = FALSE</code> no labels at all are plotted.</p>
</td></tr>
<tr><td><code id="hclust_:_axes">axes</code>, <code id="hclust_:_frame.plot">frame.plot</code>, <code id="hclust_:_ann">ann</code></td>
<td>
<p>logical flags as in <code>plot.default</code>.</p>
</td></tr>
<tr><td><code id="hclust_:_main">main</code>, <code id="hclust_:_sub">sub</code>, <code id="hclust_:_xlab">xlab</code>, <code id="hclust_:_ylab">ylab</code></td>
<td>
<p>character strings for
<code>title</code>.  <code>sub</code> and <code>xlab</code> have a non-NULL
default when there's a <code>tree$call</code>.</p>
</td></tr>
<tr><td><code id="hclust_:_...">...</code></td>
<td>
<p>Further graphical arguments.  E.g., <code>cex</code> controls
the size of the labels (if plotted) in the same way as <code>text</code>.</p>
</td></tr>
</table>


<h3>Details</h3>

<p>This function performs a hierarchical cluster analysis
using a set of dissimilarities for the <code class="reqn">n</code> objects being
clustered.  Initially, each object is assigned to its own
cluster and then the algorithm proceeds iteratively,
at each stage joining the two most similar clusters,
continuing until there is just a single cluster.
At each stage distances between clusters are recomputed
by the Lance&ndash;Williams dissimilarity update formula
according to the particular clustering method being used.
</p>
<p>A number of different clustering methods are provided.  <em>Ward's</em>
minimum variance method aims at finding compact, spherical clusters.
The <em>complete linkage</em> method finds similar clusters. The
<em>single linkage</em> method (which is closely related to the minimal
spanning tree) adopts a &lsquo;friends of friends&rsquo; clustering
strategy.  The other methods can be regarded as aiming for clusters
with characteristics somewhere between the single and complete link
methods.  Note however, that methods <code>"median"</code> and
<code>"centroid"</code> are <em>not</em> leading to a <em>monotone distance</em>
measure, or equivalently the resulting dendrograms can have so called
<em>inversions</em> or <em>reversals</em> which are hard to interpret,
but note the trichotomies in Legendre and Legendre (2012).
</p>
<p>Two different algorithms are found in the literature for Ward clustering.
The one used by option <code>"ward.D"</code> (equivalent to the only Ward
option <code>"ward"</code> in <span class="rlang"><b>R</b></span> versions <code class="reqn">\le</code> 3.0.3) <em>does not</em> implement
Ward's (1963) clustering criterion, whereas option <code>"ward.D2"</code> implements
that criterion (Murtagh and Legendre 2014).  With the latter, the
dissimilarities are <em>squared</em> before cluster updating.
Note that <code>agnes(*, method="ward")</code> corresponds
to <code>hclust(*, "ward.D2")</code>.
</p>
<p>If <code>members != NULL</code>, then <code>d</code> is taken to be a
dissimilarity matrix between clusters instead of dissimilarities
between singletons and <code>members</code> gives the number of observations
per cluster.  This way the hierarchical cluster algorithm can be
&lsquo;started in the middle of the dendrogram&rsquo;, e.g., in order to
reconstruct the part of the tree above a cut (see examples).
Dissimilarities between clusters can be efficiently computed (i.e.,
without <code>hclust</code> itself) only for a limited number of
distance/linkage combinations, the simplest one being <em>squared</em>
Euclidean distance and centroid linkage.  In this case the
dissimilarities between the clusters are the squared Euclidean
distances between cluster means.
</p>
<p>In hierarchical cluster displays, a decision is needed at each merge to
specify which subtree should go on the left and which on the right.
Since, for <code class="reqn">n</code> observations there are <code class="reqn">n-1</code> merges,
there are <code class="reqn">2^{(n-1)}</code> possible orderings for the leaves
in a cluster tree, or dendrogram.
The algorithm used in <code>hclust</code> is to order the subtree so that
the tighter cluster is on the left (the last, i.e., most recent,
merge of the left subtree is at a lower value than the last
merge of the right subtree).
Single observations are the tightest clusters possible,
and merges involving two observations place them in order by their
observation sequence number.
</p>


<h3>Value</h3>

<p>An object of class <b>hclust</b> which describes the
tree produced by the clustering process.
The object is a list with components:
</p>
<table>
<tr><td><code>merge</code></td>
<td>
<p>an <code class="reqn">n-1</code> by 2 matrix.
Row <code class="reqn">i</code> of <code>merge</code> describes the merging of clusters
at step <code class="reqn">i</code> of the clustering.
If an element <code class="reqn">j</code> in the row is negative,
then observation <code class="reqn">-j</code> was merged at this stage.
If <code class="reqn">j</code> is positive then the merge
was with the cluster formed at the (earlier) stage <code class="reqn">j</code>
of the algorithm.
Thus negative entries in <code>merge</code> indicate agglomerations
of singletons, and positive entries indicate agglomerations
of non-singletons.</p>
</td></tr>
<tr><td><code>height</code></td>
<td>
<p>a set of <code class="reqn">n-1</code> real values (non-decreasing for
ultrametric trees).
The clustering <em>height</em>: that is, the value of
the criterion associated with the clustering
<code>method</code> for the particular agglomeration.</p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p>a vector giving the permutation of the original
observations suitable for plotting, in the sense that a cluster
plot using this ordering and matrix <code>merge</code> will not have
crossings of the branches.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>labels for each of the objects being clustered.</p>
</td></tr>
<tr><td><code>call</code></td>
<td>
<p>the call which produced the result.</p>
</td></tr>
<tr><td><code>method</code></td>
<td>
<p>the cluster method that has been used.</p>
</td></tr>
<tr><td><code>dist.method</code></td>
<td>
<p>the distance that has been used to create <code>d</code>
(only returned if the distance object has a <code>"method"</code>
attribute).</p>
</td></tr>
</table>
<p>There are <code>print</code>, <code>plot</code> and <code>identify</code>
(see <code>identify.hclust</code>) methods and the
<code>rect.hclust()</code> function for <code>hclust</code> objects.
</p>


<h3>Note</h3>

<p>Method <code>"centroid"</code> is typically meant to be used with
<em>squared</em> Euclidean distances.
</p>


<h3>Author(s)</h3>

<p>The <code>hclust</code> function is based on Fortran code
contributed to STATLIB by F. Murtagh.
</p>


<h3>References</h3>

<p>Becker, R. A., Chambers, J. M. and Wilks, A. R. (1988).
<em>The New S Language</em>.
Wadsworth &amp; Brooks/Cole. (S version.)
</p>
<p>Everitt, B. (1974).
<em>Cluster Analysis</em>.
London: Heinemann Educ. Books.
</p>
<p>Hartigan, J.A. (1975).
<em>Clustering  Algorithms</em>.
New York: Wiley.
</p>
<p>Sneath, P. H. A. and R. R. Sokal (1973).
<em>Numerical Taxonomy</em>.
San Francisco: Freeman.
</p>
<p>Anderberg, M. R. (1973).
<em>Cluster Analysis for Applications</em>.
Academic Press: New York.
</p>
<p>Gordon, A. D. (1999).
<em>Classification</em>. Second Edition.
London: Chapman and Hall / CRC
</p>
<p>Murtagh, F. (1985).
&ldquo;Multidimensional Clustering Algorithms&rdquo;, in
<em>COMPSTAT Lectures 4</em>.
Wuerzburg: Physica-Verlag
(for algorithmic details of algorithms used).
</p>
<p>McQuitty, L.L. (1966).
Similarity Analysis by Reciprocal Pairs for Discrete and Continuous
Data.
<em>Educational and Psychological Measurement</em>, <b>26</b>, 825&ndash;831.
\Sexpr[results=rd]{tools:::Rd_expr_doi("10.1177/001316446602600402")}.
</p>
<p>Legendre, P. and L. Legendre (2012).
<em>Numerical Ecology</em>,
3rd English ed. Amsterdam: Elsevier Science BV.
</p>
<p>Murtagh, Fionn and Legendre, Pierre (2014).
Ward's hierarchical agglomerative clustering method: which algorithms
implement Ward's criterion?
<em>Journal of Classification</em>, <b>31</b>, 274&ndash;295.
\Sexpr[results=rd]{tools:::Rd_expr_doi("10.1007/s00357-014-9161-z")}.
</p>


<h3>See Also</h3>

<p><code>identify.hclust</code>, <code>rect.hclust</code>,
<code>cutree</code>, <code>dendrogram</code>, <code>kmeans</code>.
</p>
<p>For the Lance&ndash;Williams formula and methods that apply it generally,
see <code>agnes</code> from package <a href="https://CRAN.R-project.org/package=cluster"><span class="pkg">cluster</span></a>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>require(graphics)

### Example 1: Violent crime rates by US state

hc &lt;- hclust(dist(USArrests), "ave")
plot(hc)
plot(hc, hang = -1)

## Do the same with centroid clustering and *squared* Euclidean distance,
## cut the tree into ten clusters and reconstruct the upper part of the
## tree from the cluster centers.
hc &lt;- hclust(dist(USArrests)^2, "cen")
memb &lt;- cutree(hc, k = 10)
cent &lt;- NULL
for(k in 1:10){
  cent &lt;- rbind(cent, colMeans(USArrests[memb == k, , drop = FALSE]))
}
hc1 &lt;- hclust(dist(cent)^2, method = "cen", members = table(memb))
opar &lt;- par(mfrow = c(1, 2))
plot(hc,  labels = FALSE, hang = -1, main = "Original Tree")
plot(hc1, labels = FALSE, hang = -1, main = "Re-start from 10 clusters")
par(opar)

### Example 2: Straight-line distances among 10 US cities
##  Compare the results of algorithms "ward.D" and "ward.D2"

data(UScitiesD)

mds2 &lt;- -cmdscale(UScitiesD)
plot(mds2, type="n", axes=FALSE, ann=FALSE)
text(mds2, labels=rownames(mds2), xpd = NA)

hcity.D  &lt;- hclust(UScitiesD, "ward.D") # "wrong"
hcity.D2 &lt;- hclust(UScitiesD, "ward.D2")
opar &lt;- par(mfrow = c(1, 2))
plot(hcity.D,  hang=-1)
plot(hcity.D2, hang=-1)
par(opar)
</code></pre>

<hr /><div style="text-align: center;">[Package <em>stats</em> version 3.5.3 ]</div>
</div>
</body></html>
