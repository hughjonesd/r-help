<!DOCTYPE html><html><head><title>R: Hierarchical Clustering</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css">
<script type="text/javascript">
const macros = { "\\R": "\\textsf{R}", "\\code": "\\texttt"};
function processMathHTML() {
    var l = document.getElementsByClassName('reqn');
    for (let e of l) { katex.render(e.textContent, e, { throwOnError: false, macros }); }
    return;
}</script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js"
    onload="processMathHTML();"></script>
<link rel="stylesheet" type="text/css" href="R.css" />
</head><body> <div style='padding: 20pt; border: 2px solid black; text-align:center;'><b>This help topic is for R version 0.62.3. For the current version of R, try <a href='https://stat.ethz.ch/R-manual/R-patched/library/mva/html/hclust.html'>https://stat.ethz.ch/R-manual/R-patched/library/mva/html/hclust.html</a></b></div><div class="container"><main>

<table style="width: 100%;"><tr><td>hclust {mva}</td><td style="text-align: right;">R Documentation</td></tr></table>

<h2>Hierarchical Clustering</h2>

<h3>Description</h3>

<p>This function performs a hierarchical cluster analysis
using a set of dissimilarities for the <code class="reqn">n</code> objects being
clustered.  Initially, each object is assigned to its own
cluster and then the algorithm proceeds iteratively,
at each stage joining the two most similar clusters,
continuing until there is just a single cluster.
At each stage distances between clusters are recomputed
by the Lance-Williams dissimilarity update formula
according to the particular clustering method being used.
</p>
<p>An number of different clustering methods are provided.
Ward's minimum variance method aims at finding compact,
spherical clusters.  The complete linkage method finds
similar clusters. The single linkage method
(which is closely related to the minimal spanning tree)
adopts a &lsquo;friends of friends&rsquo; clustering strategy.
The other methods can be regarded as aiming
for clusters with characteristics somewhere between
the single and complete link methods.
</p>
<p>In hierarchical cluster displays, a decision is needed at each merge to
specify which subtree should go on the left and which on the right.
Since, for <code class="reqn">n</code> observations there are <code class="reqn">n-1</code> merges,
there are <code class="reqn">2^(n-1)</code> possible orderings for the leaves
in a cluster tree, or dendrogram.
The algorithm in <code>hclust</code> is to order the subtree so that
the tighter cluster is on the left (the last, i.e. most recent,
merge of the left subtree is at a lower value than the last
merge of the right subtree).
Observations are the tightest clusters possible,
and merges involving two observations place them in order by their
observation sequence number.
</p>


<h3>Usage</h3>

<pre><code class='language-R'>hclust(d, method="complete")

plot.hclust(hclust.obj, labels, hang=0.1, ...)
</code></pre>


<h3>Arguments</h3>

<table role = "presentation">
<tr><td><code id="d">d</code></td>
<td>
<p>a dissimilarity structure as produced by <code>dist</code>.</p>
</td></tr>
<tr><td><code id="method">method</code></td>
<td>
<p>the agglomeration method to be used. This should
be (an unambiguous abbreviation of) one of
<code>"ward"</code>, <code>"single"</code>, <code>"complete"</code>,
<code>"average"</code>, <code>"mcquitty"</code>, <code>"median"</code> or
<code>"centroid"</code>.</p>
</td></tr>
<tr><td><code id="hclust.obj">hclust.obj</code></td>
<td>
<p>an object of the type produced by <code>hclust</code>.</p>
</td></tr>
<tr><td><code id="hang">hang</code></td>
<td>
<p>The fraction of the plot height which labels should hang
below the rest of the plot.
A negative value will cause the labels to hang down from 0.</p>
</td></tr>
<tr><td><code id="labels">labels</code></td>
<td>
<p>A character vector of of labels for the leaves of the
tree. By default the row names or row numbers of the original data are
used. If <code>labels=FALSE</code> no labels at all are plotted.</p>
</td></tr>
</table>


<h3>Value</h3>

<p>An object of class <b>hclust</b> which describes the
tree produced by the clustering processs.
The object is a list with components:
</p>
<table role = "presentation">
<tr><td><code>merge</code></td>
<td>
<p>an <code class="reqn">n-1</code> by 2 matrix.
Row <code class="reqn">i</code> of <code>merge</code> describes the merging of clusters
at step <code class="reqn">i</code> of the clustering.
If an element <code class="reqn">j</code> in the row is negative,
then observation <code class="reqn">-j</code> was merged at this stage.
If <code class="reqn">j</code> is positive then the merge
was with the cluster formed at the (earlier) stage <code class="reqn">j</code>
of the algorithm.
Thus negative entries in <code>merge</code> indicate agglomerations
of singletons, and positive entries indicate agglomerations
of non-singletons.</p>
</td></tr>
<tr><td><code>height</code></td>
<td>
<p>a set of <code class="reqn">n-1</code> non-decreasing real values.
The clustering <em>height</em>: that is, the value of
the criterion associated with the clustering
<code>method</code> for the particular agglomeration.</p>
</td></tr>
<tr><td><code>order</code></td>
<td>
<p>a vector giving the permutation of the original observations suitable for
plotting, in the sense that a cluster plot using this ordering
and matrix <code>merge</code> will not have crossings of the branches.</p>
</td></tr>
<tr><td><code>labels</code></td>
<td>
<p>labels for each of the objects being clustered.</p>
</td></tr>
</table>


<h3>Author(s)</h3>

<p>The <code>hclust</code> function is based on Fortran code
contributed to STATLIB by F. Murtagh.</p>


<h3>References</h3>

<p>Everitt, B. (1974).
<em>Cluster Analysis</em>.
London: Heinemann Educ. Books.
</p>
<p>Hartigan, J. A. (1975).
<em>Clustering  Algorithms</em>.
New York: Wiley.
</p>
<p>Sneath, P. H. A. and R. R. Sokal (1973).
<em>Numerical Taxonomy</em>.
San Francisco: Freeman.
</p>
<p>Anderberg, M. R. (1973).
<em>Cluster Analysis for Applications</em>.
Academic Press: New York.
</p>
<p>Gordon, A. D. (1981).
<em>Classification</em>.
London: Chapman and Hall.
</p>
<p>Murtagh, F. (1985).
&ldquo;Multidimensional Clustering Algorithms&rdquo;, in
<em>COMPSTAT Lectures 4</em>.
Wuerzburg: Physica-Verlag
(for algorithmic details of algorithms used).
</p>


<h3>See Also</h3>

<p><code>kmeans</code>.
</p>


<h3>Examples</h3>

<pre><code class='language-R'>library(mva)
data(crimes)
hc &lt;- hclust(dist(crimes), "ave")
plot(hc, hang=-1)
plot(hc)
</code></pre>

</main>

</div>
</body></html>
